# **Explainable Artificial Intelligence: Principles, Techniques, and the Imperative for Understanding Model Decisions**

The integration of Artificial Intelligence (AI) and Machine Learning (ML) into a multitude of applications has become increasingly pervasive, transforming industries and impacting daily life 1. These technologies are deployed to tackle intricate problems across diverse sectors, including healthcare, where AI aids in diagnostics; finance, where it assesses risk; and even within the legal system 1. As the sophistication of ML models advances, particularly with the rise of deep neural networks, their inherent complexity often results in systems that function as "black boxes" 2. While these complex models can achieve high levels of accuracy, their opacity presents a significant impediment to establishing trust and ensuring transparency in their operation 1. This lack of understandability can hinder the widespread adoption of AI, especially in critical domains where the rationale behind decisions carries substantial weight. Explainable AI (XAI) has emerged as a crucial field dedicated to addressing this challenge by providing methods and processes that enable humans to comprehend and have confidence in the outputs generated by AI systems 6.

## **Defining Explainable AI and Related Concepts**

### **Explainable AI (XAI)**

Explainable Artificial Intelligence encompasses a range of techniques and methodologies designed to make the outcomes and decision-making processes of AI systems understandable to humans 6. It serves to elucidate how AI models arrive at their predictions, thereby fostering trust among users and stakeholders 6. The core objective of XAI is to demystify the "black box" nature often associated with complex AI algorithms 3. By providing insights into the factors influencing an AI model's output, XAI helps in describing the model itself, its anticipated impact, and any potential biases it might harbor 6. Furthermore, it plays a vital role in characterizing crucial aspects of the model, such as its accuracy, fairness, transparency, and the overall outcomes it produces 8. In essence, XAI acts as a form of "cognitive translation," bridging the gap between the intricate logic of machine intelligence and the human need for understanding and reason 3.

### **Interpretability**

Interpretability in AI refers to the capacity to comprehend how a machine learning model arrives at its decisions 1. This involves the ability to trace the sequence of steps within the model that lead to a particular prediction and to discern which input features exert the most influence 1. While closely related to explainability, interpretability focuses more on the mechanics of the model's operation – the "how" behind its predictions – whereas explainability aims to provide the "why," the reasons or justifications for those predictions 1. Interpretability can be viewed as a foundational element for achieving explainability in many cases 9. Different facets of interpretability exist, including whether it is intrinsic to the model's design (like in decision trees) or applied after training (post-hoc), and whether it seeks to understand individual predictions (local) or the model's overall behavior (global) 14.

### **Transparency**

Transparency in the context of AI refers to the degree of openness in the design, development, and deployment of AI systems 15. A transparent AI system is one where its underlying mechanisms, the sources of data it utilizes, and its decision-making processes are readily accessible and understandable 16. This openness is crucial for building trust in AI technologies and for ensuring their fairness and ethical application 3. Transparency can manifest at various levels, including understanding the algorithms used (algorithmic transparency), the interactions between users and the system (interaction transparency), and the broader societal implications of AI deployment (social transparency) 17.

The concepts of Explainability, Interpretability, and Transparency, while often used in conjunction, represent a spectrum of understanding AI systems. Transparency provides the overall visibility into the system's workings. Interpretability allows for the comprehension of the model's internal logic in producing outputs. Explainability then offers human-understandable reasons for specific decisions or predictions. This layered understanding is fundamental to the responsible development and deployment of AI.

## **The Importance of Understanding Machine Learning Model Decisions**

Comprehending the decision-making processes of ML models through XAI is paramount for a multitude of reasons that span technical, ethical, and business domains 1.

A primary concern is **responsibility and accountability** in the use of AI 5. In scenarios where AI systems make consequential decisions, understanding the basis for those decisions is essential for establishing accountability 5. This is particularly relevant in regulated industries where organizations must be able to justify their AI-driven actions 19.

Furthermore, understanding AI decisions is crucial for **building trust and confidence** among users and stakeholders 5. When individuals understand how an AI system arrives at a particular outcome, they are more likely to trust its recommendations and integrate it into their workflows 5. This trust is vital for the successful adoption of AI technologies across various applications 8.

Interpretability and explainability also **facilitate the adaptation and improvement** of AI models 8. By understanding which features are most influential and how they contribute to predictions, developers can identify areas for model refinement, leading to enhanced performance and reliability 5.

In an increasingly regulated landscape, XAI plays a key role in **ensuring regulatory compliance** 5. Many data protection and AI ethics regulations mandate greater transparency in the decision-making processes of AI models. Interpretability and explainability are essential for adhering to these requirements and demonstrating that AI systems operate within legal and ethical boundaries 3.

Understanding how AI models function also enables the **reduction of bias and the promotion of fairness** 5. By making the decision-making process transparent, XAI helps in identifying potential biases in the training data or the algorithms themselves, allowing for corrective measures to be implemented 3. This is crucial for ensuring that AI systems do not perpetuate or amplify societal inequalities 8.

Moreover, interpretability serves as a valuable tool for **debugging and error detection** in ML models 7. By understanding the reasoning behind a model's predictions, developers can pinpoint areas of weakness or misinterpretation, leading to more accurate and robust systems 1.

Explainable AI can also lead to an **improved user experience** 8. When end-users understand why an AI system is recommending a particular product or service, they are more likely to trust and engage with it 8.

Beyond these practical benefits, interpretability **facilitates knowledge transfer** among stakeholders 14. When the workings of a model are understood, it becomes easier to share insights and leverage that knowledge in other model development efforts 14. In research settings, XAI can even enable **scientific discovery** by revealing previously unknown relationships within the data 20.

The ability to understand the decisions made by machine learning models is not merely a desirable attribute but a fundamental requirement for the responsible and effective deployment of AI across a wide spectrum of applications. The multifaceted benefits of XAI highlight its pivotal role in shaping the future of artificial intelligence.

## **Principles of Explainable AI**

Several fundamental principles underpin the development and evaluation of Explainable AI systems, ensuring they are trustworthy and understandable 3.

**Transparency** is a core principle, emphasizing that the processes within an AI system should be comprehensible to humans 3. This involves having clarity on how the model works, what data it uses, and how it combines these elements to produce outputs 16.

**Interpretability** focuses on ensuring that the outputs of an AI model are understandable to users, even those without specialized technical knowledge 3. Techniques like visualizations and natural language processing can enhance the interpretability of complex AI processes 15.

**Explainability** itself is a key principle, requiring that an AI system can provide clear and meaningful reasons or evidence for its decisions and outputs 3. These explanations should be in a format that is accessible and understandable to the intended audience 10.

**Justifiability** demands that AI decisions are not only explainable but also substantiated with evidence that can be reviewed and validated 12. This is particularly important for meeting regulatory requirements and ensuring the ethical deployment of AI 15.

**Robustness and Reliability** are crucial, ensuring that explainable AI applications maintain consistent and dependable performance even when faced with variations in input data or changing environments 15.

**Causality** delves into understanding the cause-and-effect relationships within the AI model, identifying which factors directly influence the outcomes 23.

**Fairness** is an increasingly important principle, emphasizing the need to identify and mitigate biases within AI systems to ensure equitable outcomes for all users 8.

Beyond these, the National Institute of Standards and Technology (NIST) has outlined additional principles, including the need for explanations to be **meaningful** to the target audience, for the explanations to have **accuracy** in reflecting the system's processes, and for AI systems to operate within their designated **knowledge limits**, flagging cases they are not designed to handle 10.

These principles collectively form a guiding framework for the development of XAI, ensuring that AI systems are not only powerful but also understandable, accountable, and aligned with human values and expectations.

## **Techniques for Explainable AI**

The landscape of Explainable AI encompasses a diverse array of techniques, which can broadly be categorized into model-specific and model-agnostic approaches 7.

### **Model-Specific Explainability**

Model-specific techniques are inherently tied to the architecture and characteristics of particular types of machine learning models 26. These methods leverage the internal structure of the model to provide insights into its decision-making process 28.

Intrinsically interpretable models, such as **decision trees**, offer a clear and transparent view of how decisions are made through their hierarchical tree structure 7. The path from the root to a leaf node represents a set of rules that lead to a specific prediction, making the reasoning readily understandable 1. Similarly, **linear regression** models provide interpretability through their coefficients, which directly indicate the weight and direction of each feature's influence on the predicted outcome 14. **Rule-based models**, where decisions are based on a predefined set of if-then rules, are also inherently interpretable as the logic is explicitly encoded 7.

For more complex model types, specific techniques exist to enhance understanding. In **tree-based models** like Random Forests and Gradient Boosting Machines, **feature importance** scores can be calculated based on how frequently and at what depth features are used in the trees, providing a measure of their influence on the model's predictions 1. For **neural networks**, **activation visualization** techniques allow researchers to examine the patterns learned by different layers and neurons, offering insights into the features the network has identified as important 25. Additionally, efforts are underway to develop **rule extraction** methods that can distill the complex relationships learned by black-box models into a set of human-readable rules 1.

### **Model-Agnostic Explainability**

In contrast to model-specific methods, model-agnostic techniques can be applied to any machine learning model, regardless of its underlying architecture 27. These methods treat the model as a "black box," focusing on the relationship between input features and output predictions 29.

One widely used model-agnostic technique is **permutation feature importance**. This method assesses the importance of a feature by randomly shuffling its values in the dataset and observing the resulting impact on the model's performance. A significant drop in performance indicates that the permuted feature is important for the model's predictions 30.

**Partial Dependence Plots (PDPs)** are another valuable tool for global model interpretation. They visualize the average effect of one or two features on the model's predicted outcome, allowing users to understand the general relationship between the selected features and the prediction 31. **Individual Conditional Expectation (ICE) curves** provide a more granular view by showing the dependence of the prediction on a feature for each individual instance in the dataset 31.

The concept of **surrogate models** involves training a simpler, interpretable model (such as a linear model or a decision tree) to approximate the predictions of a more complex black-box model 31. By analyzing the interpretable surrogate model, insights into the behavior of the original model can be gained 12.

**Counterfactual explanations** offer a different perspective by identifying the smallest changes to an input instance that would result in a different prediction 7. These explanations help users understand what needs to change to achieve a desired outcome.

Two particularly powerful and popular model-agnostic techniques are SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations).

**SHAP** is based on game theory and calculates Shapley values to assign an importance score to each feature for a particular prediction 32. These values represent the contribution of each feature to the difference between the actual prediction and the average prediction 34. SHAP offers several useful properties, including additivity, local accuracy, and consistency 32. It also provides various visualization methods, such as waterfall plots that show the contribution of each feature to a single prediction, force plots that offer a similar view in a different format, summary plots that provide an overview of feature importance across the entire dataset, and dependence plots that illustrate the relationship between a feature's value and its SHAP value 35.

**LIME** focuses on explaining individual predictions by approximating the black-box model locally with an interpretable surrogate model, typically a linear model 36. For a given instance, LIME generates a set of perturbed samples, obtains predictions from the black-box model for these samples, and then trains a weighted interpretable model on this local neighborhood 38. The weights assigned to the perturbed samples are based on their proximity to the original instance, ensuring that the local model accurately reflects the behavior of the black-box model in that specific region 39. The coefficients of the local linear model then serve as explanations for the prediction 40.

| Feature | Model-Specific Techniques | Model-Agnostic Techniques |
| :---- | :---- | :---- |
| Applicability | Limited to specific model types | Applicable to any ML model |
| Access to Model Internals | Requires access to model architecture and parameters | Treats the model as a black box |
| Insight Depth | Can provide deeper insights into model workings | Focuses on input-output relationships |
| Flexibility | Less flexible, tailored to specific models | More flexible, can be used across different models |
| Examples | Decision Trees, Linear Regression, Feature Importance in Trees, Activation Visualization in Neural Networks | Permutation Feature Importance, Partial Dependence Plots, LIME, SHAP, Surrogate Models, Counterfactual Explanations, Individual Conditional Expectation (ICE) Curves |

## **Local vs. Global Explainability**

Explainability techniques can also be categorized based on their scope: local or global 41.

**Local explainability** focuses on understanding why a machine learning model made a specific prediction for an individual instance 41. The primary question it seeks to answer is, "Why did the model predict this particular outcome?" 43. Techniques like LIME and SHAP are prominent examples of local explanation methods, as they provide insights into the feature contributions for a single data point 41. The strength of local explainability lies in its precision and actionability for individual cases, offering specific reasons for a particular decision and potentially suggesting changes to alter that outcome 42. However, local explanations do not necessarily provide a comprehensive understanding of the model's overall behavior across the entire dataset 42.

**Global explainability**, on the other hand, aims to understand the overall behavior and decision-making logic of the model across the entire dataset 19. It seeks to answer the broader question, "How does the model behave in general?" 42. Techniques such as permutation feature importance and partial dependence plots fall under the umbrella of global explainability, as they provide insights into the average impact of features on the model's predictions across all instances 19. Global explanations offer a "big picture" view of the model's reliance on different features and can aid in auditing and strategic decisions regarding model development and deployment 42. A limitation of global explainability is that it may conceal important nuances in how the model behaves for specific subgroups or individual instances 42.

| Feature | Local Explainability | Global Explainability |
| :---- | :---- | :---- |
| Scope | Individual predictions | Entire dataset/model behavior |
| Question Answered | Why this specific prediction? | How does the model behave overall? |
| Techniques | LIME, SHAP, Counterfactual Explanations | Permutation Feature Importance, Partial Dependence Plots |
| Strengths | Precision, actionability | Big picture view, auditing |
| Limitations | Limited scope, potential instability | Averages can mislead, no individual reasoning |

Often, a combined approach leveraging both local and global explainability techniques is most effective for gaining a comprehensive understanding of a machine learning model. Starting with a global overview can reveal general patterns and biases, while then zooming in on specific instances with local explanations can provide detailed insights into individual decisions 42.

## **Conclusion**

Explainable Artificial Intelligence is a critical field that addresses the increasing need for transparency and understandability in AI and Machine Learning systems. As AI continues to permeate various aspects of society, the ability to comprehend how these systems make decisions becomes paramount for building trust, ensuring fairness, and complying with regulations. This report has explored the fundamental concepts of XAI, including its relationship with interpretability and transparency. It has highlighted the multifaceted importance of understanding ML model decisions, spanning from ensuring accountability to facilitating model improvement and mitigating biases. The core principles that guide the development of XAI systems, such as transparency, interpretability, explainability, justifiability, robustness, and fairness, provide a framework for building trustworthy AI. A diverse range of techniques, categorized as model-specific and model-agnostic, offer various ways to achieve explainability, catering to different types of models and analytical goals. Furthermore, the distinction between local and global explainability underscores the importance of considering the scope of the explanation required, whether it's for understanding individual predictions or the overall behavior of the model. The ongoing advancements in Explainable AI are essential for the responsible evolution and widespread adoption of artificial intelligence, ensuring that these powerful technologies are both effective and comprehensible to the humans who design, deploy, and are impacted by them.

#### **Works cited**

1. The Importance of Understanding How Machine Learning Models Make Decisions, accessed March 30, 2025, [https://eagerminds.medium.com/the-importance-of-understanding-how-machine-learning-models-make-decisions-3b1bfd839407?source=author\_recirc-----9b40c0940ce7----0----------------------------](https://eagerminds.medium.com/the-importance-of-understanding-how-machine-learning-models-make-decisions-3b1bfd839407?source=author_recirc-----9b40c0940ce7----0----------------------------)  
2. Decoding Machine Learning Model Decisions: Tools for Interpretation \- MarkovML, accessed March 30, 2025, [https://www.markovml.com/blog/interpretation-tool](https://www.markovml.com/blog/interpretation-tool)  
3. What Is Explainable AI (XAI)? \- Palo Alto Networks, accessed March 30, 2025, [https://www.paloaltonetworks.com/cyberpedia/explainable-ai](https://www.paloaltonetworks.com/cyberpedia/explainable-ai)  
4. What is Explainability? | C3 AI Glossary Definition, accessed March 30, 2025, [https://c3.ai/glossary/machine-learning/explainability/](https://c3.ai/glossary/machine-learning/explainability/)  
5. Interpretability vs explainability: Understanding the Differences and Importance in the World of Artificial Intelligence \- XCALLY, accessed March 30, 2025, [https://www.xcally.com/news/interpretability-vs-explainability-understanding-the-importance-in-artificial-intelligence/](https://www.xcally.com/news/interpretability-vs-explainability-understanding-the-importance-in-artificial-intelligence/)  
6. www.ibm.com, accessed March 30, 2025, [https://www.ibm.com/think/topics/explainable-ai\#:\~:text=Explainable%20artificial%20intelligence%20(XAI)%20is,expected%20impact%20and%20potential%20biases.](https://www.ibm.com/think/topics/explainable-ai#:~:text=Explainable%20artificial%20intelligence%20\(XAI\)%20is,expected%20impact%20and%20potential%20biases.)  
7. What is Explainable AI? Benefits & Best Practices \- Qlik, accessed March 30, 2025, [https://www.qlik.com/us/augmented-analytics/explainable-ai](https://www.qlik.com/us/augmented-analytics/explainable-ai)  
8. What is Explainable AI (XAI)? \- IBM, accessed March 30, 2025, [https://www.ibm.com/think/topics/explainable-ai](https://www.ibm.com/think/topics/explainable-ai)  
9. EXPLAINABLE ARTIFICIAL INTELLIGENCE \- European Data Protection Supervisor, accessed March 30, 2025, [https://www.edps.europa.eu/system/files/2023-11/23-11-16\_techdispatch\_xai\_en.pdf](https://www.edps.europa.eu/system/files/2023-11/23-11-16_techdispatch_xai_en.pdf)  
10. What Is Explainable AI (XAI)? | Built In, accessed March 30, 2025, [https://builtin.com/artificial-intelligence/explainable-ai](https://builtin.com/artificial-intelligence/explainable-ai)  
11. www.paloaltonetworks.com, accessed March 30, 2025, [https://www.paloaltonetworks.com/cyberpedia/ai-explainability\#:\~:text=Explainability%20in%20artificial%20intelligence%20refers,AI%20decisions%20transparent%20and%20trustworthy.](https://www.paloaltonetworks.com/cyberpedia/ai-explainability#:~:text=Explainability%20in%20artificial%20intelligence%20refers,AI%20decisions%20transparent%20and%20trustworthy.)  
12. Explainable AI Principles: What Should You Know About XAI \- ITRex Group, accessed March 30, 2025, [https://itrexgroup.com/blog/explainable-ai-principles-classification-examples/](https://itrexgroup.com/blog/explainable-ai-principles-classification-examples/)  
13. Explainable vs. Interpretable Artificial Intelligence \- Splunk, accessed March 30, 2025, [https://www.splunk.com/en\_us/blog/learn/explainability-vs-interpretability.html](https://www.splunk.com/en_us/blog/learn/explainability-vs-interpretability.html)  
14. What Is AI Interpretability? \- IBM, accessed March 30, 2025, [https://www.ibm.com/think/topics/interpretability](https://www.ibm.com/think/topics/interpretability)  
15. The 4 Key Principles of Explainable AI Applications \- Virtualitics, accessed March 30, 2025, [https://virtualitics.com/the-4-key-principles-of-explainable-ai-applications/](https://virtualitics.com/the-4-key-principles-of-explainable-ai-applications/)  
16. www.zendesk.com, accessed March 30, 2025, [https://www.zendesk.com/blog/ai-transparency/\#:\~:text=AI%20transparency%20means%20understanding%20how,trust%20how%20these%20systems%20work.](https://www.zendesk.com/blog/ai-transparency/#:~:text=AI%20transparency%20means%20understanding%20how,trust%20how%20these%20systems%20work.)  
17. What is AI transparency? A comprehensive guide \- Zendesk, accessed March 30, 2025, [https://www.zendesk.com/blog/ai-transparency/](https://www.zendesk.com/blog/ai-transparency/)  
18. Transparency, Explainability, and Interpretability of AI \- Cimplifi, accessed March 30, 2025, [https://www.cimplifi.com/resources/transparency-explainability-and-interpretability-of-ai/](https://www.cimplifi.com/resources/transparency-explainability-and-interpretability-of-ai/)  
19. Explainability in Machine Learning \- Take Control of ML and AI Complexity \- Seldon, accessed March 30, 2025, [https://www.seldon.io/explainability-in-machine-learning-2/](https://www.seldon.io/explainability-in-machine-learning-2/)  
20. 2 Interpretability – Interpretable Machine Learning, accessed March 30, 2025, [https://christophm.github.io/interpretable-ml-book/interpretability.html](https://christophm.github.io/interpretable-ml-book/interpretability.html)  
21. Crucial Concepts in AI: Transparency and Explainability \- F5, accessed March 30, 2025, [https://www.f5.com/company/blog/crucial-concepts-in-ai-transparency-and-explainability](https://www.f5.com/company/blog/crucial-concepts-in-ai-transparency-and-explainability)  
22. Explainable AI, LIME & SHAP for Model Interpretability | Unlocking AI's Decision-Making, accessed March 30, 2025, [https://www.datacamp.com/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models](https://www.datacamp.com/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models)  
23. What are the 4 principles of Explainable AI? \- Integrail, accessed March 30, 2025, [https://integrail.ai/blog/what-are-the-4-principles-of-explainable-ai](https://integrail.ai/blog/what-are-the-4-principles-of-explainable-ai)  
24. 4 Principles of Explainable AI and How to Implement Them \- Kolena, accessed March 30, 2025, [https://www.kolena.com/guides/4-principles-of-explainable-ai-and-how-to-implement-them/](https://www.kolena.com/guides/4-principles-of-explainable-ai-and-how-to-implement-them/)  
25. Explainability in AI and Machine Learning Systems: An Overview \- Comet.ml, accessed March 30, 2025, [https://www.comet.com/site/blog/explainability-in-ai-and-machine-learning-systems-an-overview/](https://www.comet.com/site/blog/explainability-in-ai-and-machine-learning-systems-an-overview/)  
26. Explainable Artificial Intelligence (XAI) for AI & ML Engineers \- DataScienceCentral.com, accessed March 30, 2025, [https://www.datasciencecentral.com/explainable-artificial-intelligence-xai-for-ai-ml-engineers/](https://www.datasciencecentral.com/explainable-artificial-intelligence-xai-for-ai-ml-engineers/)  
27. 3.2 Taxonomy of Interpretability Methods | Interpretable Machine Learning, accessed March 30, 2025, [https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html](https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html)  
28. Explainability In Machine Learning: Top Techniques \- Arize AI, accessed March 30, 2025, [https://arize.com/blog-course/explainability-techniques-shap/](https://arize.com/blog-course/explainability-techniques-shap/)  
29. What are Model Agnostic Methods?. You might be wondering, “What exactly… | by Amit Yadav | Biased-Algorithms | Medium, accessed March 30, 2025, [https://medium.com/biased-algorithms/what-are-model-agnostic-methods-ef9f6f697a83](https://medium.com/biased-algorithms/what-are-model-agnostic-methods-ef9f6f697a83)  
30. Explainable AI: A Comprehensive Review of the Main Methods | MLearning.ai \- Medium, accessed March 30, 2025, [https://medium.com/@dallanoce.fd/explainable-ai-a-complete-summary-of-the-main-methods-a28f9ab132f7](https://medium.com/@dallanoce.fd/explainable-ai-a-complete-summary-of-the-main-methods-a28f9ab132f7)  
31. 4 Methods Overview – Interpretable Machine Learning \- Christoph Molnar, accessed March 30, 2025, [https://christophm.github.io/interpretable-ml-book/overview.html](https://christophm.github.io/interpretable-ml-book/overview.html)  
32. An Introduction to SHAP Values and Machine Learning Interpretability \- DataCamp, accessed March 30, 2025, [https://www.datacamp.com/tutorial/introduction-to-shap-values-machine-learning-interpretability](https://www.datacamp.com/tutorial/introduction-to-shap-values-machine-learning-interpretability)  
33. SHapley Additive exPlanations or SHAP : What is it ? \- DataScientest.com, accessed March 30, 2025, [https://datascientest.com/en/shap-what-is-it](https://datascientest.com/en/shap-what-is-it)  
34. 18 SHAP – Interpretable Machine Learning, accessed March 30, 2025, [https://christophm.github.io/interpretable-ml-book/shap.html](https://christophm.github.io/interpretable-ml-book/shap.html)  
35. shap/shap: A game theoretic approach to explain the output of any machine learning model. \- GitHub, accessed March 30, 2025, [https://github.com/shap/shap](https://github.com/shap/shap)  
36. LIME: Local Interpretable Model-Agnostic Explanations \- C3 AI, accessed March 30, 2025, [https://c3.ai/glossary/data-science/lime-local-interpretable-model-agnostic-explanations/](https://c3.ai/glossary/data-science/lime-local-interpretable-model-agnostic-explanations/)  
37. Exploring LIME (Local Interpretable Model-agnostic Explanations) — Part 1 | by Sze Zhong LIM | Data And Beyond | Medium, accessed March 30, 2025, [https://medium.com/data-and-beyond/exploring-lime-local-interpretable-model-agnostic-explanations-part-1-3934ec6e96dc](https://medium.com/data-and-beyond/exploring-lime-local-interpretable-model-agnostic-explanations-part-1-3934ec6e96dc)  
38. How LIME works | Understanding in 5 steps \- Openlayer, accessed March 30, 2025, [https://www.openlayer.com/blog/post/understanding-lime-in-5-steps](https://www.openlayer.com/blog/post/understanding-lime-in-5-steps)  
39. 14 LIME – Interpretable Machine Learning, accessed March 30, 2025, [https://christophm.github.io/interpretable-ml-book/lime.html](https://christophm.github.io/interpretable-ml-book/lime.html)  
40. LIME: explain Machine Learning predictions | by Giorgio Visani | TDS Archive \- Medium, accessed March 30, 2025, [https://medium.com/towards-data-science/lime-explain-machine-learning-predictions-af8f18189bfe](https://medium.com/towards-data-science/lime-explain-machine-learning-predictions-af8f18189bfe)  
41. Explainable AI (XAI): Artificial Intelligence Explained \- Netguru, accessed March 30, 2025, [https://www.netguru.com/glossary/explainable-ai](https://www.netguru.com/glossary/explainable-ai)  
42. Local vs. Global Explainability: Why Both Matter for Interpreting Your ML Models \- 123 of AI, accessed March 30, 2025, [https://www.123ofai.com/post/local-vs-global-explainability](https://www.123ofai.com/post/local-vs-global-explainability)  
43. What is Global, Cohort and Local Explainability? | Censius AI Observability Blog, accessed March 30, 2025, [https://censius.ai/blogs/global-local-cohort-explainability](https://censius.ai/blogs/global-local-cohort-explainability)